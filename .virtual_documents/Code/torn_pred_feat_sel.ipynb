


# Load necessary libraries. 
import os 
import tqdm
import random 
import numpy as np 
import pandas as pd 
import sklearn.metrics as metrics
import shap 

# import our models 
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor 
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier 


threshold = 45          # can change threshold at certain window ... 
nc_path = "Archive/data/"
csv_path = "Archive/"
cls_names = { "torn":0, "wind":1, "hail":2 }

vars = [ 'temp', 'humidity', 'dew', 'precip', 'preciptype', 'windspeed', 'winddir', 'pressure', 'cloudcover', 'visibility']
stat_vars = [ "mean", "max", "min", "var", "nonzeros", "above_threshold" ]
vars += stat_vars

def read_data( csv, vars, set='train' ):
    dframe = pd.read_csv( csv )
    x = dframe[ dframe['set']==set ][vars]
    y = dframe[ dframe['set']==set ]['label'].values        
    return x,y 


# Choose the Data we will use for training and test.
csv_file = "balanced_comb.csv"
train_x, train_y = read_data( csv_path + "/" + csv_file, vars, set="train" )
print( f"The number of data points in train set:\n", train_x.shape, train_y.shape )
print( type(train_x), type(train_y) ) 
val_x, val_y = read_data( csv_path + "/" + csv_file, vars, set="val" )
print( f"The number of data points in val set:\n", val_x.shape, val_y.shape )
test_x, test_y = read_data( csv_path + "/" + csv_file, vars, set="test" )
print( f"The number of data points in test set:\n", test_x.shape, test_y.shape )

# train_x, train_y, val_x, val_y, test_x, test_y = read_data( csv_path + "/" + csv_file, vars, random_balanced = True )  
# print( f"The number of data points in train set:\n", train_x.shape, train_y.shape )





import matplotlib.pyplot as plt 
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix
from sklearn.metrics import ConfusionMatrixDisplay 

def evaluate( model, train_x, train_y, test_x, test_y ):
    """ uses the model to calculate a set of scores """
    # print('Test Accuracy     :',model.score(test_x, test_y))
    # print('Training Accuracy    :',model.score(train_x,train_y))
    pred_y = model.predict(test_x)
    accuracy = accuracy_score( test_y, pred_y )
    precision = precision_score( test_y, pred_y, average='weighted', zero_division=True )
    recall = recall_score( test_y, pred_y, average='weighted' )
    f1 = f1_score( test_y, pred_y, average='weighted')
    print('Testing accuracy :', accuracy )
    print('Testing precision :', precision )
    print('Testing recall    :', recall )
    print('F1 score          :', f1 )
    # ROC AUC
    probs=model.predict_proba(test_x) 
    print('ROC AUC           : %f' % roc_auc_score(test_y, probs, average="weighted", multi_class='ovo'))
    conf_mat = confusion_matrix( test_y, pred_y, normalize='true' )
    disp = ConfusionMatrixDisplay(conf_mat, display_labels=["torn", "wind", "hail"])
    disp = disp.plot(cmap=plt.cm.Blues)   # ,values_format='g'
    plt.show()





from operator import itemgetter
from boruta import BorutaPy   # doesn't work for now ... 
from shaphypetune import BoostBoruta
from sklearn.feature_selection import RFE
from feature_engine.selection import RecursiveFeatureAddition   # the only RFA I've found ... 





print('====================== Original Logistic Regression =======================')
lg = LogisticRegression(C=0.01,penalty='l2',max_iter=30000, 
                           solver="lbfgs",multi_class='multinomial', 
                           random_state=0)

lg.fit(train_x.values,train_y) 
evaluate(lg, train_x.values, train_y, test_x.values, test_y) 

# print('====================== Logistic Regression with Boruta =======================')
# lg = LogisticRegression(C=0.01,penalty='l2', max_iter=30000, solver="lbfgs",
#                            multi_class='multinomial', random_state=0)
# 
# rfe_lg = RFE(lg, n_features_to_select=12)
# rfe_lg.fit( train_x, train_y )
# evaluate(rfe_lg, train_x, train_y, test_x, test_y) 
# features = train_x.columns.to_list()


print('====================== Logistic Regression with RFE =======================')
lg = LogisticRegression(C=0.01,penalty='l2', max_iter=30000, solver="lbfgs",
                           multi_class='multinomial', random_state=0)

rfe_lg = RFE(lg, n_features_to_select=12)
rfe_lg.fit( train_x, train_y )
evaluate(rfe_lg, train_x, train_y, test_x, test_y) 
features = train_x.columns.to_list()
for x, y in (sorted(zip(rfe_lg.ranking_ , features), key=itemgetter(0))):
    print(x, y)

print('======================= Logistic Regression with RFA =======================')
lg = LogisticRegression(C=0.01,penalty='l2', max_iter=30000, solver="lbfgs",
                           multi_class='multinomial', random_state=0)
rfa_lg = RecursiveFeatureAddition(estimator=lg, scoring="accuracy", cv=3)
vars = rfa_lg.fit_transform(train_x, train_y) 
vars = vars.columns.tolist() 
print(vars)
print( rfa_lg.initial_model_performance_ )
print( rfa_lg.performance_drifts_ ) 

lg = LogisticRegression(C=0.01,penalty='l2', max_iter=30000, solver="lbfgs",
                           multi_class='multinomial', random_state=0)
lg.fit( train_x[vars], train_y )
evaluate(lg, train_x[vars], train_y, test_x[vars], test_y) 





print('======================= Original Random Forest =======================')

rfc = RandomForestClassifier( n_estimators= 100, max_depth=15, min_samples_split=110,
                                 min_samples_leaf=25,max_features='sqrt' ,oob_score=True,random_state=10 )
rfc.fit(train_x.values, train_y)
evaluate(rfc, train_x.values, train_y, test_x.values, test_y)


# print('======================= Random Forest with Boruta =======================')
# rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)
# boruta_selector = BorutaPy(rf, random_state=0)
# boruta_selector.fit_transform(train_x, train_y)
# selected_features = train_x.columns[boruta_selector.support_]

# print(f"Number of selected features: {len(selected_features)}")
# print(f"Selected features: {list(selected_features)}")


print('============================= Random Forest with RFE =============================')

rfc = RandomForestClassifier( n_estimators= 100, max_depth=15, min_samples_split=110,
                            min_samples_leaf=25,max_features='sqrt' ,oob_score=True,random_state=10 )
rfe_rfc = RFE(rfc, n_features_to_select=12)
rfe_rfc.fit( train_x, train_y )
evaluate(rfe_rfc, train_x, train_y, test_x, test_y) 
features = train_x.columns.to_list()
for x, y in (sorted(zip(rfe_rfc.ranking_ , features), key=itemgetter(0))):
    print(x, y)


print('============================= Random Forest with RFA =============================')
rfc = RandomForestClassifier( n_estimators= 100, max_depth=15, min_samples_split=110,
                            min_samples_leaf=25,max_features='sqrt' ,oob_score=True,random_state=10 )
rfa_rfc = RecursiveFeatureAddition(estimator=rfc, scoring="accuracy", cv=3)
vars = rfa_rfc.fit_transform(train_x, train_y) 
vars = vars.columns.tolist() 
print(vars)
print( rfa_rfc.initial_model_performance_ ) 
print( rfa_rfc.performance_drifts_ ) 

rfc = RandomForestClassifier( n_estimators= 100, max_depth=15, min_samples_split=110,
                            min_samples_leaf=25,max_features='sqrt' ,oob_score=True,random_state=10 )
rfc.fit( train_x[vars], train_y )
evaluate(rfc, train_x[vars], train_y, test_x[vars], test_y) 

# print('============================= Random Forest with Boruta =============================')
# feat_selector = BorutaPy(
#     verbose=2,
#     estimator=rfc,
#     n_estimators=100 )

# feat_selector.fit(np.array(train_x, dtype=float), np.array(train_y, dtype=int))
# evaluate(feat_selector, train_x.values, train_y, test_x.values, test_y) 





print('============================= Original LightGBM =============================')
lgb = LGBMClassifier( n_estimators=500, verbose=-1 )
lgb.fit(train_x, train_y)
evaluate( lgb, train_x, train_y, test_x, test_y )

print('============================= LightGBM with Boruta =============================')
lgb = LGBMClassifier( n_estimators=500, verbose=-1 )
boruta_lgb = BoostBoruta(lgb, max_iter=20, perc=100)
boruta_lgb.fit(train_x, train_y, eval_set=[(val_x, val_y)], eval_metric='multi_logloss')
print(boruta_lgb.estimator_, boruta_lgb.n_features_)
print(boruta_lgb.ranking_)
evaluate(boruta_lgb, train_x, train_y, test_x, test_y)

print('============================= LightGBM with RFE =============================')
lgb = LGBMClassifier( n_estimators=500, verbose=-1 )
rfe_lgb = RFE(lgb, n_features_to_select=12)
rfe_lgb.fit( train_x, train_y )
evaluate(rfe_lgb, train_x, train_y, test_x, test_y) 
features = train_x.columns.to_list()
for x, y in (sorted(zip(rfe_lgb.ranking_ , features), key=itemgetter(0))):
    print(x, y)

print('============================= LightGBM with RFA =============================')
lgb = LGBMClassifier( n_estimators=500, verbose=-1 )  
rfa_lgb = RecursiveFeatureAddition(estimator=lgb, scoring="accuracy", cv=3)
vars = rfa_lgb.fit_transform(train_x, train_y) 
vars = vars.columns.tolist() 
print(vars)
print( rfa_lgb.initial_model_performance_ ) 
print( rfa_lgb.performance_drifts_ ) 

lgb = LGBMClassifier( n_estimators=5000, verbose=-1 )  
lgb.fit( train_x[vars], train_y )
evaluate(lgb, train_x[vars], train_y, test_x[vars], test_y) 





print('============================= Original XGBoost =============================')
xgb = XGBClassifier( n_estimators = 500)
xgb.fit( train_x, train_y ) 
evaluate( xgb, train_x, train_y, test_x, test_y )

print('============================= XGBoost with Boruta =============================')
lgb = LGBMClassifier( n_estimators=500, verbose=-1 )
boruta_lgb = BoostBoruta(lgb, max_iter=40, perc=100)
boruta_lgb.fit(train_x, train_y, eval_set=[(val_x, val_y)], eval_metric='multi_logloss')
print(boruta_lgb.estimator_, boruta_lgb.n_features_)
print(boruta_lgb.ranking_)
evaluate(boruta_lgb, train_x, train_y, test_x, test_y)

print('============================= XGBoost with RFE =============================')
xgb = XGBClassifier( )
rfe_xgb = RFE(xgb, n_features_to_select=12)
rfe_xgb.fit( train_x, train_y )
evaluate( rfe_xgb, train_x, train_y, test_x, test_y )
features = train_x.columns.to_list()
for x, y in (sorted(zip(rfe_xgb.ranking_ , features), key=itemgetter(0))):
    print(x, y)

print('============================= XGBoost with RFA =============================')
xgb = XGBClassifier( )
rfa_xgb = RecursiveFeatureAddition(estimator=xgb, scoring="accuracy", cv=3)
vars = rfa_xgb.fit_transform(train_x, train_y) 
vars = vars.columns.tolist() 
print(vars)
print( rfa_lgb.initial_model_performance_ ) 
print( rfa_lgb.performance_drifts_ ) 

xgb = LGBMClassifier( )  
xgb.fit( train_x[vars], train_y )
evaluate(xgb, train_x[vars], train_y, test_x[vars], test_y) 



